{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8: Nearest Neighbors Regression üèò"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Jeff Che\n",
    "\n",
    "Student ID: 464957\n",
    "\n",
    "Collaborators: Andrew Wu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this homework, we will be exploring a more realistic application of similarity-based leanring. It might be helpful to review **Lab 8 (Feature Transformation and Similarity-based Prediction with k-NN)** first. Most of the things we ask you to do in this homework are explained in the lab. In general, you should feel free to import any package that we have previously used in class. Ensure that all plots have the necessary components that a plot should have (e.g. axes labels, a title, a legend).\n",
    "\n",
    "Furthermore, in addition to recording your collaborators on this homework, please also remember to cite/indicate all external sources used when finishing this assignment. This includes peers, TAs, and links to online sources. Note that these citations will not free you from your obligation to submit your _own_ code and write-ups, however, they will be taken into account during the grading and regrading process.\n",
    "\n",
    "### Submission instructions\n",
    "* Submit this python notebook including your answers in the code cells as homework submission.\n",
    "* **Feel free to add as many cells as you need to** ‚Äî just make sure you don't change what we gave you. \n",
    "* **Does it spark joy?** Note that you will be partially graded on the presentation (_cleanliness, clarity, comments_) of your notebook so make sure you [Marie Kondo](https://lifehacker.com/marie-kondo-is-not-a-verb-1833373654) your notebook before submitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using `sklearn` for $k$-Nearest Neighbors\n",
    "\n",
    "In Lab 8, we got familiar with $k$-nearest neighbors ($k$-NN) by implementing the algorithm. If you are still not comfortable with how the algorithm works, then we suggest that you review your work from the lab. We will proceed here under this assumption.\n",
    "\n",
    "In this section, we will explore how to use the [$k$-NN _regression_ model supplied by `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor). You can find the [$k$-NN _classification_ model here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Some Data\n",
    "\n",
    "We'll need to start by getting some data ‚Äî what is data science without data? For this assignment, we will be revisiting another old friend: the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're here, let's review what this dataset is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1\n",
    "\n",
    "**Write-up!** How many examples are in the dataset? How many features does it have? What are the features? What is the target variable that we would like to estimate? What kind of machine learning problem is this?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "\n",
    "there are 506 examples and 13 features each. The features are:\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per $10,000\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in $1000's\n",
    "        \n",
    "The target variable that I want to estimate is the price. This is a regression machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data\n",
    "\n",
    "In the lab, we also looked at data scaling and transformations. Here we'll demonstrate how to use `sklearn` to help us with this. Let's call this **approach 1**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# new train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# compute the mean and standard deviation on a training set \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# apply the transforamtion to both the trainnig and the test set\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative and much quicker way of scaling the the data is the following (let's call this **approach 2**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# scaler?\n",
    "# X_scaled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2\n",
    "\n",
    "**Write-up** What types of scaling does `StandardScaler()` and `scale` perform? `Hint` Use the [`?` operator](https://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html#accessing-help). Which of the two proceedures is a more appropriate preprocessing step for supervised machine learning and _why_? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "\n",
    "The standard scaler conforms everything by unit variance and mean, instead of a min-max (range), which is scale. \n",
    "\n",
    "knn is a distance based algorithm and standard scale works better so it brings all unit variances and means down to the same level. Thus, relevant magnitudes of features will impact the algorithm less. This is generally more appropriate for supervised as well since we have control of our input data. It is worthwhile to scale everything and make the model reflect a truer result of what the data is showing; less errors. This \"better\" result is reflected in section 3 of this assignment; scaled data produces much higher r^2 scores, suggesting that the regression is more accurate for the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking Into the Model\n",
    "\n",
    "Now that we have some data to play with, let's try building a $k$-NN regression model. The model provided by `sklearn` shares the a similar interface as the other models that we have looked at previously (esp. $k$-means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        KNeighborsRegressor\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                    weights='uniform')\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/_regression.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Regression based on k-nearest neighbors.\n",
       "\n",
       "The target is predicted by local interpolation of the targets\n",
       "associated of the nearest neighbors in the training set.\n",
       "\n",
       "Read more in the :ref:`User Guide <regression>`.\n",
       "\n",
       ".. versionadded:: 0.9\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_neighbors : int, optional (default = 5)\n",
       "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
       "\n",
       "weights : str or callable\n",
       "    weight function used in prediction.  Possible values:\n",
       "\n",
       "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
       "      are weighted equally.\n",
       "    - 'distance' : weight points by the inverse of their distance.\n",
       "      in this case, closer neighbors of a query point will have a\n",
       "      greater influence than neighbors which are further away.\n",
       "    - [callable] : a user-defined function which accepts an\n",
       "      array of distances, and returns an array of the same shape\n",
       "      containing the weights.\n",
       "\n",
       "    Uniform weights are used by default.\n",
       "\n",
       "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
       "    Algorithm used to compute the nearest neighbors:\n",
       "\n",
       "    - 'ball_tree' will use :class:`BallTree`\n",
       "    - 'kd_tree' will use :class:`KDTree`\n",
       "    - 'brute' will use a brute-force search.\n",
       "    - 'auto' will attempt to decide the most appropriate algorithm\n",
       "      based on the values passed to :meth:`fit` method.\n",
       "\n",
       "    Note: fitting on sparse input will override the setting of\n",
       "    this parameter, using brute force.\n",
       "\n",
       "leaf_size : int, optional (default = 30)\n",
       "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
       "    speed of the construction and query, as well as the memory\n",
       "    required to store the tree.  The optimal value depends on the\n",
       "    nature of the problem.\n",
       "\n",
       "p : integer, optional (default = 2)\n",
       "    Power parameter for the Minkowski metric. When p = 1, this is\n",
       "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
       "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
       "\n",
       "metric : string or callable, default 'minkowski'\n",
       "    the distance metric to use for the tree.  The default metric is\n",
       "    minkowski, and with p=2 is equivalent to the standard Euclidean\n",
       "    metric. See the documentation of the DistanceMetric class for a\n",
       "    list of available metrics.\n",
       "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
       "    must be square during fit. X may be a :term:`Glossary <sparse graph>`,\n",
       "    in which case only \"nonzero\" elements may be considered neighbors.\n",
       "\n",
       "metric_params : dict, optional (default = None)\n",
       "    Additional keyword arguments for the metric function.\n",
       "\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    The number of parallel jobs to run for neighbors search.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "    Doesn't affect :meth:`fit` method.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "effective_metric_ : string or callable\n",
       "    The distance metric to use. It will be same as the `metric` parameter\n",
       "    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
       "    'minkowski' and `p` parameter set to 2.\n",
       "\n",
       "effective_metric_params_ : dict\n",
       "    Additional keyword arguments for the metric function. For most metrics\n",
       "    will be same with `metric_params` parameter, but may also contain the\n",
       "    `p` parameter value if the `effective_metric_` attribute is set to\n",
       "    'minkowski'.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> X = [[0], [1], [2], [3]]\n",
       ">>> y = [0, 0, 1, 1]\n",
       ">>> from sklearn.neighbors import KNeighborsRegressor\n",
       ">>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
       ">>> neigh.fit(X, y)\n",
       "KNeighborsRegressor(...)\n",
       ">>> print(neigh.predict([[1.5]]))\n",
       "[0.5]\n",
       "\n",
       "See also\n",
       "--------\n",
       "NearestNeighbors\n",
       "RadiusNeighborsRegressor\n",
       "KNeighborsClassifier\n",
       "RadiusNeighborsClassifier\n",
       "\n",
       "Notes\n",
       "-----\n",
       "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
       "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
       "   neighbors, neighbor `k+1` and `k`, have identical distances but\n",
       "   different labels, the results will depend on the ordering of the\n",
       "   training data.\n",
       "\n",
       "https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3\n",
    "\n",
    "Use the [`?` operator](https://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html#accessing-help) provided by IPython to explore `model` and it's interface.\n",
    "\n",
    "**Try this!** In the cell below, complete the following:\n",
    "1. create and fit a new `KNeighborsRegressor` model with 5 neighbors\n",
    "2. make some predictions using the model on your testing data\n",
    "3. evaluate the performance of the model by computing $R^2$ and storing it in `r_squared`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared Value:\n",
      "0.7153853569196222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# your code here\n",
    "#r-squared function from hw 4\n",
    "def r2(predictions, labels):\n",
    "    '''computes the r-squared metric of a model given some PREDICTIONS and their true LABELS'''\n",
    "    \n",
    "    # your code here\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    sum_mean = 0\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        sum_mean += labels[i]\n",
    "    \n",
    "    y_mean = sum_mean/len(labels)\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        sum1 += (labels[i] - predictions[i])**2\n",
    "        sum2 += (labels[i] - y_mean)**2\n",
    "        \n",
    "    r2 = 1 - (sum1/sum2)\n",
    "    \n",
    "    assert np.isscalar(r2), 'R2 should be a scalar value'\n",
    "    \n",
    "    return r2\n",
    "\n",
    "newModel = KNeighborsRegressor(n_neighbors=5) #new model\n",
    "newModel.fit(X_train, y_train)\n",
    "y_pred = newModel.predict(X_test) #predictions\n",
    "\n",
    "r_squared = r2(y_pred, y_test)\n",
    "\n",
    "assert np.isclose(r_squared, 0.71538, rtol=1e-4), 'You should see this R^2 value'\n",
    "\n",
    "print(\"R-Squared Value:\")\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4\n",
    "\n",
    "**Write-up!** What was the $R^2$ value for your $k$-NN model using five neighbors? What does $R^2$ tell you about a model? What does this score tell you about your model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "\n",
    "My r^2 is 0.715385 for kNN using five neighbors. This score tells us essentially a measure of how close the data are to the fitted regression line. This tells us that it is pretty good, the closer to 1 the score is, the better. ~71% of points correctly reflected by regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, let's move on to some more interesting things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choosing $k$ with Cross Validation\n",
    "\n",
    "In order to test whether the `kNN` algorithm (or any other machine learning algorithm) performs how we want it to and accurately makes predictions, we must compare the known label of all datapoints to the predicted label of those same datapoints. So far we have seen this in the forms of model evaluation and validation in model selection. \n",
    "\n",
    "In model evaluation we partitioned our original dataset into two parts: a training set and a testing set. As we have seen earlier in the course, the testing set is a smaller percentage of the total dataset than the training set.\n",
    "\n",
    "Later on, in model selection, we explored why it was important to have yet another set of data partitioned out for usage as a validation set, which we could use to experiment with a model's hyperparameters. The validation set allowed use to \"evaluate\" our model's performance with various settings of it's parameters while maintaining a completely untouched dataset for out final evaluation.\n",
    "\n",
    "We can extend this idea once again to improve our estimates of model performance through **cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `kFolds` method\n",
    "\n",
    "One version of cross validation partitions the dataset into `k` partitions, or folds. We use `k-1` folds to train the model, then the one fold we left out to test the model. We iterate this process `k` times, leaving out a different fold each time, so that we have an accuracy score for each one of the `k` different partitions. We can then take the average of all of these accuracies to calculate a more wholistic accuracy representation of the algorithm. In the example below, `k = 5`; there are 5 partitions. Each partition is used once as a test partition while the other 4 are used for training purposes. The idea for $k$-fold cross validation is based on the realization that we can get a better picture of our model's performance by feeding it many combinations of our data.\n",
    "\n",
    "![](utility/pics/kFold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `KFold` function to partition our dataset into `k` partitions. While the `KFold` function does not split the dataset itself, it provides the indices by which to split the dataset.\n",
    "\n",
    "Below, we split an arbitrary array of length 10 into 5 folds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 0: Train indices: [0 1 2 3 4 5 8 9]. Test indices: [6 7]\n",
      "For iteration 1: Train indices: [0 1 2 3 4 6 7 8]. Test indices: [5 9]\n",
      "For iteration 2: Train indices: [0 1 3 4 5 6 7 9]. Test indices: [2 8]\n",
      "For iteration 3: Train indices: [0 2 3 5 6 7 8 9]. Test indices: [1 4]\n",
      "For iteration 4: Train indices: [1 2 4 5 6 7 8 9]. Test indices: [0 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "dummy = np.arange(10) # example data\n",
    "\n",
    "# initialize KFolds\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# iterating over k different splits of dummy\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(dummy)):\n",
    "    print(f'For iteration {fold}: Train indices: {train_idx}. Test indices: {test_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each testing datapoint appears once, ensuring that all datapoints have had a chance to be be tested against the model trained with the rest of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1\n",
    "\n",
    "Now, let's try using the `KFold` operation on the full Boston Housing dataset, building and fitting new $k$-NN models with each fold, and averaging the scores of each model.\n",
    "\n",
    "**Try this!** Complete the `knn_kfolds` function so that it performs `n_folds`-fold cross validation of $k$-NN models on `X` using `n_neighbors` and returns the average $R^2$ value of the models in `avg_score`.\n",
    "\n",
    "* Make sure to scale your training and test sets appropriately (√† la the [Scaling Data](#Scaling-Data) section).\n",
    "* Ensure that you make and fit a new model for each fold.\n",
    "* Also, please make sure that you set `random_state` appropriately in your initialization of `KFold`.\n",
    "\n",
    "`Hint` Refer to the previous example of how to use `KFold` and your work in [Problem 1.2](#Problem-1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5143691791619528"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_kfolds(X, y, n_folds, n_neighbors, random_state=None):\n",
    "    '''Computes'''\n",
    "\n",
    "    # your code here\n",
    "    kf2 = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n",
    "    total = 0\n",
    "    runs = 0\n",
    "    \n",
    "    for fold, (train, test) in enumerate(kf2.split(X)):\n",
    "        newModel2 = KNeighborsRegressor(n_neighbors=n_neighbors) # new model and predictions for each fold\n",
    "        newModel2.fit(X[train], y[train])\n",
    "        y_pred2 = newModel2.predict(X[test])\n",
    "        total = total + r2(y_pred2, y[test])\n",
    "        runs = runs + 1\n",
    "    \n",
    "    avg_score = total/runs #avg r squared of all\n",
    "    \n",
    "    assert np.isscalar(avg_score), 'The average score should be a single number'\n",
    "    assert 0 <= avg_score and avg_score <= 1, 'The average score should be between 0 and 1'\n",
    "    \n",
    "    return avg_score\n",
    "\n",
    "knn_kfolds(X, y, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the above function: avg r squared score using X, y, 5 folds, and 5 neighbors: 0.514."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $k$\n",
    "\n",
    "We can use cross validation as a substitute for the model selection algorithm that we've used in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2\n",
    "\n",
    "In this problem, we will use cross validation and our `knn_kfolds` function to help us pick the right $k$ to use for our Boston Housing predictions.\n",
    "\n",
    "**Try this!** In the following cell, use 10-fold cross validation to evaluate the performance of $k$-NN on $X_{\\text{scaled}}$ and $y$ from the Boston Housing dataset and provide a plot of the cross validation average $R^2$ values for $k$ values from 1 to 20 (inclusive). Use a random state of 12 for your analysis. Ensure that your plot has the appropriate components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8057347674767061\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r2s = []\n",
    "x_axis = []\n",
    "for value in range(1, 21):\n",
    "    r2s.append(knn_kfolds(X_scaled, y, 10, value, random_state=12))\n",
    "    x_axis.append(value)\n",
    "    \n",
    "plt.plot(x_axis, r2s)\n",
    "plt.xlabel('K Value (integer values only)')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.title('K vs R^2 Score (scaled)')\n",
    "\n",
    "print(r2s[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg r squared score for k=2: 0.806."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3\n",
    "\n",
    "**Write-up!** Based on your plot from [Problem 2.2](#Problem-2.2), which $k$ value would you pick for your final model? Explain why."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "\n",
    "I would choose k=2, it gives the highest score compared to the rest in the plot; it is ~80% of points are accuratey reflected by the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection with Cross Validation\n",
    "\n",
    "As mentioned before, we can use cross validation to get a more thorough evaluation of model performance. This means that we can use if for model selection by substituting it for the validation set process that we have used in the past.\n",
    "\n",
    "In this section, we will compare our $k$-NN regression model with a linear regression model that we used back in Lab 4 when we last looked at the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1\n",
    "\n",
    "**Try this!** In the following cell, report the cross validation score (average $R^2$) of a $k$-NN model with the $k$ you selected in [Problem 2.3](#Problem-2.3) on `X_scaled`. Use a random state of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8014346481907209\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "score = knn_kfolds(X_scaled, y, 10, 2, random_state=4)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation score for k=2 on X_scaled: 0.801."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2\n",
    "\n",
    "Now let's do 10-fold cross validation on a linear regression model on `X` without scaling.\n",
    "\n",
    "**Write-up** Why should shouldn't we use scaling here? What will happen if we do?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "\n",
    "should use scaling, scaling removes the possible error from features all having different magnitudes. Similar idea as discussed in section 1.2. If we don't use scaling, errors can throw off results, and this is seen in a lower r^2 overall; data points fit regression (the predictions) less often. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Perform 10-fold cross validation for linear regression on `X` and report the average $R^2$ value across all of the folds. Make sure to create and fit new models for each fold of the process. Use a random state of 5. `Hint` Refer to your work in [Problem 2.1](#Problem-2.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 r^2: 0.653456915851263\n",
      "model 2 r^2: 0.32524180231247113\n",
      "model 3 r^2: 0.4806496043975488\n",
      "model 4 r^2: 0.6301990030436511\n",
      "model 5 r^2: 0.39994787251889075\n",
      "model 6 r^2: 0.5885293761479784\n",
      "model 7 r^2: 0.6584079619530475\n",
      "model 8 r^2: 0.5812607652215978\n",
      "model 9 r^2: 0.6452681392298933\n",
      "model 10 r^2: 0.42425400443561934\n",
      "avg across all models: 0.5387215445111961\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "kf3 = KFold(n_splits=10, random_state=5, shuffle=True)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train, test) in enumerate(kf3.split(X)):\n",
    "    newModel3 = KNeighborsRegressor(n_neighbors=2)\n",
    "    newModel3.fit(X[train], y[train])\n",
    "    y_pred3 = newModel3.predict(X[test])\n",
    "    fold_scores.append(r2(y_pred3, y[test]))\n",
    "    \n",
    "total_score = 0\n",
    "count = 0\n",
    "for each_score in fold_scores:\n",
    "    total_score = total_score + each_score\n",
    "    count = count + 1\n",
    "    print(f\"model {count} r^2: {each_score}\")\n",
    "    \n",
    "avgOfAll = total_score/count\n",
    "print(f\"avg across all models: {avgOfAll}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.3\n",
    "\n",
    "**Write-up!** What were the $R^2$ values for each of the models? Which model would you prefer? Why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "\n",
    "Avg across ALL models: 0.5387215445111961\n",
    "\n",
    "model 1 r^2: 0.653456915851263\n",
    "model 2 r^2: 0.32524180231247113\n",
    "model 3 r^2: 0.4806496043975488\n",
    "model 4 r^2: 0.6301990030436511\n",
    "model 5 r^2: 0.39994787251889075\n",
    "model 6 r^2: 0.5885293761479784\n",
    "model 7 r^2: 0.6584079619530475\n",
    "model 8 r^2: 0.5812607652215978\n",
    "model 9 r^2: 0.6452681392298933\n",
    "model 10 r^2: 0.42425400443561934\n",
    "\n",
    "I would prefer model 7 because it has the highest r^2 score, compared to the rest. Fits regression best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.4\n",
    "\n",
    "**Write-up!** Describe your next steps as a data scientist now that you have decided which model to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "\n",
    "Now that we have determined a model that is best from testing, can test the model more against new data. Can then tweak the model accordingly (potentially training more, trying/testing on different data sets, etc.). Can expand data set by collecting more to run model on. After we are adequately satisfied with performance, could then possibly prepare model for real world application in the housing market to help predict prices (integration into user interfaces, consumer use, realtor use, etc.). Models performance in a real world application could then show us weaknesses, etc. that would require more tweaking and can develop and improve upon it further (likely with the introduction of more data and other features/relevant aspects that can affect pricing). I think this goes to show how dynamic everything can be and a data scientist can be constantly improving something."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
